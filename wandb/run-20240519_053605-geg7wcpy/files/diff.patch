diff --git a/datasets.py b/datasets.py
index 3ac9fff..69f916c 100644
--- a/datasets.py
+++ b/datasets.py
@@ -49,59 +49,6 @@ class A2D2_symmetric(Dataset):
 
     
 
-
-
-
-class AllDatasets2(Dataset): #calculate mean here
-    def __init__(self, dataset1, dataset2,batch):
-        # self.transform
-        self.dataset1 = dataset1
-        self.dataset2 = dataset2
-        self.batch=int(batch)
-
-    def __len__(self):
-        return int((len(self.dataset1) + len(self.dataset2))/self.batch)-1
-
-    def __getitem__(self, index):
-        image = torch.empty((self.batch, 3, 224, 224))
-        segmentation = torch.empty((self.batch,43, 224, 224))
-        ### ### ### $$$ $$$
-
-        if index*self.batch < len(self.dataset1):
-            steering = torch.empty((self.batch, 21))
-            for i in range(0,self.batch):
-
-                image[i] = self.dataset1[index*self.batch + i]['image']
-                segmentation[i] = torch.from_numpy(self.dataset1[index*self.batch + i]['segmentation']) #torch.from_numpy(
-                steering[i] = self.dataset1[index*self.batch + i]['steering']
-                
-            return {'image':image, 'segmentation': segmentation, 'steering': steering }
-        else:
-            index=index-(len(self.dataset1)/index*self.batch)
-            steering =  torch.empty((self.batch, 0)) 
-
-            for i in range(0,self.batch):
-                image[i] = self.dataset2[index*self.batch + i]['image']
-                segmentation[i] = self.dataset2[index*self.batch + i ]['segmentation']
-                steering[i] = torch.tensor([], dtype=torch.float32)
-
-            return  {'image':image, 'segmentation': segmentation, 'steering': steering }
-
-class AllDatasets(Dataset): #calculate mean here
-    def __init__(self, dataset1, dataset2):
-        # self.transform
-        self.dataset1 = dataset1
-        self.dataset2 = dataset2
-
-    def __len__(self):
-        return len(self.dataset1) + len(self.dataset2)
-
-    def __getitem__(self, index):
-        if index < len(self.dataset1):
-            return self.dataset1[index]
-        else:
-            return self.dataset2[index - len(self.dataset1)]
-
 class A2D2_steering(Dataset):#standarilization 
     def __init__(self, path):
         self.image_paths = path
@@ -173,85 +120,6 @@ class A2D2_seg(Dataset): #21 steering angle and segmentation
        
 
 
-class DatasetA2D2(Dataset): #21 steering angle and segmentation
-    def __init__(self, path):
-       self.image_paths = path
-       self.transforms = transforms.Compose([transforms.Resize((256, 256)),
-                                             transforms.ToTensor()])
-       self.transforms_seg = transforms.Compose([transforms.ToTensor()
-                                            ])
-       
-   
-    def __len__(self):
-        return len(self.image_paths)
-
-    def __getitem__(self, index):
-        img_path=self.image_paths[index]
-        seg_path=img_path.replace('/camera/', '/multi_label/').replace('_camera_', '_label_').replace('.png','.npy')
-        # json_path=self.convert_path(img_path)  
-        json_path=img_path.replace('/camera_lidar_semantic/', '/bus/').replace('/camera/cam_front_center/','/bus/').replace('png','json')
-
-
-        if not os.path.exists(json_path):
-            steering_angles = np.zeros(21, dtype=np.float32)
-        else:
-            with open(json_path) as json_file:
-                json_data = json.load(json_file)
-                steering_angles = json_data.get('steering_angles')
-        
-                if steering_angles is None or not steering_angles:
-                    steering_angles = np.zeros(21, dtype=np.float32)
-
-            json_file.close()
-        
-        
-        steering=torch.tensor(steering_angles[-10:], dtype=torch.float32)
-        steering/=70
-        
-        image = Image.open(img_path).convert('RGB') 
-        image=self.transforms(image)
-
-        segmentation = np.load(seg_path)#.astype(np.float32)
-
-        
-             
-
-
-
-        return {'image':image, 'segmentation': segmentation, 'steering': steering }
-    
-
-
-        
-
-
-class DatasetKitty(Dataset): #image and segmentation
-    def __init__(self, path):
-        self.image_paths = path
-        self.transforms = transforms.Compose([transforms.Resize((224, 224)),
-                                              transforms.ToTensor()])
-
-    
-    def __len__(self):
-        return len(self.image_paths) 
-
-
-    def __getitem__(self, index):
-        img_path=self.image_paths[index]
-        seg_path=img_path.replace('/image_2/', '/semantic_rgb/')
-
-
-        image = Image.open(img_path)
-        segmentation = Image.open(seg_path)
-
-        image=self.transforms(image)
-        segmentation=self.transforms(segmentation)
-
-        steering=[]
-        steering=torch.tensor(steering, dtype=torch.float32)
-        return {'image':image, 'segmentation': segmentation, 'steering': steering }
-
-
 class A2D2_box(Dataset):
     def __init__(self, path):
         # grid x grid x ()
@@ -370,21 +238,56 @@ class A2D2_depth(Dataset):
         # depth = cv2.cvtColor(depth, cv2.COLOR_BGR2GRAY)
 
         return {'dt_label':'A2D2_depth','image':image,'A2D2_depth': depth}
-       
-# BASE_PATH = sorted(glob.glob("/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/*/camera/cam_front_center/*.png"))
+    
+
 
-# test_dt=A2D2_depth(BASE_PATH)
+def a2d2_dataloader(base_path="./Datasets/camera_lidar_semantic_bboxes/", train_split=0.8, val_split=0.2):
+    insult_list=['./Datasets/camera_lidar_semantic_bboxes/20180810_142822/camera/cam_front_center/20180810142822_camera_frontcenter_000000004.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000020.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000022.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000041.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000049.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000050.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000055.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000057.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000063.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000035.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000051.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000055.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000061.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181108_103155/camera/cam_front_center/20181108103155_camera_frontcenter_000000028.png',
+             './Datasets/camera_lidar_semantic_bboxes/20181108_103155/camera/cam_front_center/20181108103155_camera_frontcenter_000000033.png'
+             ]
 
+    insult_list=[os.path.abspath(path) for path in insult_list]
 
-# import time
-# 
-# start_time = time.time()
-# sample=test_dt[1]
-# end_time = time.time()
 
 
+    all_folders = sorted(os.listdir(base_path))
+    all_folders = all_folders[0:-3]  # Adjust according to your specific needs
+    
+    A2D2_path_train = [] 
+    A2D2_path_val = []
 
-# cv2.imwrite('depth_test_interpolated.png', d)
+    for folder in all_folders:
+        folder_path = os.path.join(base_path, folder)
+        
+        # Get a list of all files in the current folder
+        files_in_folder = np.array(sorted(glob.glob(os.path.join(folder_path, "camera/cam_front_center/*.png"))))
+        files_in_folder = files_in_folder[~np.isin(files_in_folder, insult_list)]
+        
+        # Calculate the split indices
+        split_index = int(len(files_in_folder) * train_split)
+        
+        # Split the data into training and validation sets for the current folder
+        train_set = files_in_folder[:split_index]
+        val_set = files_in_folder[split_index:]
+        
+        # Accumulate the sets for each folder
+        A2D2_path_train.extend(train_set)
+        A2D2_path_val.extend(val_set)
+    
+    return A2D2_path_train, A2D2_path_val
+
+       
 
-# print("Test time:",end_time-start_time)
-# print("Depth shape", sample["depth"].shape)
\ No newline at end of file
diff --git a/train_depth.py b/train_depth.py
index 62f4557..024fe51 100644
--- a/train_depth.py
+++ b/train_depth.py
@@ -15,7 +15,7 @@ import torchvision.transforms.functional as F
 import torchvision.models as models
 # from skimage.util import random_noise
 import glob
-from datasets import A2D2_steering,A2D2_seg,A2D2_depth
+from datasets import A2D2_steering,A2D2_seg,A2D2_depth,a2d2_dataloader
 from losses import YOLOLoss,DepthLoss,MaskedMSELoss,MaskedL1Loss
 
 import random
@@ -45,95 +45,21 @@ filename = f"weights/depth_{current_time}.pth"
 
 BATCH_SIZE = 4
 
-TRAIN_SPLIT = 0.8
-VAL_SPLIT = 0.2
-
-insult_list=['/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20180810_142822/camera/cam_front_center/20180810142822_camera_frontcenter_000000004.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000020.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000022.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000041.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000049.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000050.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000055.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000057.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132300/camera/cam_front_center/20181107132300_camera_frontcenter_000000063.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000035.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000051.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000055.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_000000061.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181108_103155/camera/cam_front_center/20181108103155_camera_frontcenter_000000028.png',
-             '/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/20181108_103155/camera/cam_front_center/20181108103155_camera_frontcenter_000000033.png']
-
-
-insult_list = np.array(insult_list)
-BASE_PATH = "/gpfs/space/home/alimahar/hydra/Datasets/A2D2/semantic/camera_lidar_semantic/"
-
-# Get a list of all folders in the base path
-all_folders = sorted(os.listdir(BASE_PATH))
-all_folders = all_folders[1:-3]
-print(all_folders)
-# exit()
-
-A2D2_path_train_seg=[]
-A2D2_path_train_str=[]
-
-A2D2_path_val_seg = []
-A2D2_path_val_str = [] 
-
-A2D2_path_train  = [] 
-A2D2_path_val = []
-
-for folder in all_folders:
-    folder_path = os.path.join(BASE_PATH, folder)
-    
-    # Get a list of all files in the current folder
-    files_in_folder = np.array(sorted(glob.glob(os.path.join(folder_path, "camera/cam_front_center/*.png"))))
-    files_in_folder = files_in_folder[~np.isin(files_in_folder, insult_list)]
-
-    
-    # Shuffle the list of files
-    # random.shuffle(files_in_folder)
-    
-    # Calculate the split indices
-    split_index = int(len(files_in_folder) * TRAIN_SPLIT)
-    
-    # Split the data into training and validation sets for the current folder
-    train_set = files_in_folder[:split_index]
-    val_set = files_in_folder[split_index:]
-    
-    # Accumulate the sets for each folder
-    A2D2_path_train.extend(train_set)
-    A2D2_path_val.extend(val_set)
-
-    A2D2_path_train_seg.extend(train_set[::2])
-    A2D2_path_train_str.extend(train_set[1::2])
 
-    A2D2_path_val_seg.extend(val_set[::2])
-    A2D2_path_val_str.extend(val_set[1::2])
 
+A2D2_path_train,A2D2_path_val=a2d2_dataloader()
 
 
-
-A2D2_path_train_seg=A2D2_path_train[:int(len(A2D2_path_train)/2)]
-A2D2_path_train_str=A2D2_path_train[int(len(A2D2_path_train)/2):]
-
-A2D2_path_val_seg= A2D2_path_val[:int(len(A2D2_path_val)/2)]
-A2D2_path_val_str= A2D2_path_val[int(len(A2D2_path_val)/2):]
-
-A2D2_dataset_train_seg=A2D2_depth(A2D2_path_train_seg)
-# A2D2_dataset_train_str=A2D2_steering(A2D2_path_train_str)
-
-A2D2_dataset_val_seg=A2D2_depth(A2D2_path_val_seg)
-# A2D2_dataset_val_str=A2D2_steering(A2D2_path_val_str)
+A2D2_dataset_train_seg=A2D2_depth(A2D2_path_train)
+A2D2_dataset_val_seg=A2D2_depth(A2D2_path_val)
 
 
 
 
 device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
-train_dataset = A2D2_dataset_train_seg    #ConcatDataset([A2D2_dataset_train_seg,A2D2_dataset_train_str])      #AllDatasets2(A2D2_dataset_train,Kitty_dataset_train,BATCH_SIZE)
-val_dataset = A2D2_dataset_val_seg #  ConcatDataset([A2D2_dataset_val_seg,A2D2_dataset_val_str])      #AllDatasets2(A2D2_dataset_val,Kitty_dataset_val,BATCH_SIZE)
-
+train_dataset = A2D2_dataset_train_seg        
+val_dataset = A2D2_dataset_val_seg  
 
 print('No of train samples', len(train_dataset))
 print('No of validation Samples', len(val_dataset))
@@ -217,12 +143,8 @@ for epoch in range(n_epochs):
 
 
         wandb.log({
-                #   "Training Loss ": loss,
-                #   "Training Steering Loss ": steering_loss_value,
+
                   "Training Depth Loss" : depth_loss_value
-                #   "Average Total Training Loss (During Epoch)": total_training_loss / (i + 1), 
-                #   "Average Training Steering Loss (During Epoch)": training_steering_loss / (i + 1),
-                #   "Average Training depth Loss (During Epoch)": training_depth_loss / (i + 1)
                 })
         
 
diff --git a/train_segmentation.py b/train_segmentation.py
index a4e7286..6060014 100644
--- a/train_segmentation.py
+++ b/train_segmentation.py
@@ -114,8 +114,6 @@ best_val_loss=999999
 config = {
     "learning_rate": lr,
     "batch_segmentation": BATCH_SIZE
-
-
 }
 
 
